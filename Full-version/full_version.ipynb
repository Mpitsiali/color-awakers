{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "Here I tried to make the code compatible to Tensorflow 2(like with aplha and beta version). You can try to run it in the original tf-1 version of the code\n",
    "\n",
    "I used a subset of imagenet  and trained the model but the results didn't color at all. In the beta version I got better results.\n",
    "Maybe with more training it can do better than beta version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.layers import Conv2D, UpSampling2D, InputLayer, Conv2DTranspose, Input, Reshape, concatenate\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import RepeatVector, Permute\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray, gray2rgb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave,imshow\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'test_image_net/'\n",
    "train_dir = 'train_image_net/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get images\n",
    "X = []\n",
    "for filename in os.listdir('train_image_net/'):\n",
    "    if filename =='.DS_Store':\n",
    "        continue\n",
    "    X.append(img_to_array(load_img('train_image_net/'+filename)))\n",
    "X = np.array(X, dtype=float)\n",
    "Xtrain = 1.0/255*X\n",
    "\n",
    "\n",
    "#Load weights\n",
    "inception = InceptionResNetV2(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_input = Input(shape=(1000,))\n",
    "\n",
    "#Encoder\n",
    "encoder_input = Input(shape=(256, 256, 1,))\n",
    "encoder_output = Conv2D(64, (3,3), activation='relu', padding='same', strides=2)(encoder_input)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(128, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same', strides=2)(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(512, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "encoder_output = Conv2D(256, (3,3), activation='relu', padding='same')(encoder_output)\n",
    "\n",
    "#Fusion\n",
    "fusion_output = RepeatVector(32 * 32)(embed_input) \n",
    "fusion_output = Reshape(([32, 32, 1000]))(fusion_output)\n",
    "fusion_output = concatenate([encoder_output, fusion_output], axis=3) \n",
    "fusion_output = Conv2D(256, (1, 1), activation='relu', padding='same')(fusion_output) \n",
    "\n",
    "#Decoder\n",
    "decoder_output = Conv2D(128, (3,3), activation='relu', padding='same')(fusion_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(64, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "decoder_output = Conv2D(32, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(16, (3,3), activation='relu', padding='same')(decoder_output)\n",
    "decoder_output = Conv2D(2, (3, 3), activation='tanh', padding='same')(decoder_output)\n",
    "decoder_output = UpSampling2D((2, 2))(decoder_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input, embed_input], outputs=decoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_train_function.<locals>.train_function at 0x3d38fa020> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x43ec9e710>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_inception_embedding(grayscaled_rgb):\n",
    "    grayscaled_rgb_resized = [resize(i, (299, 299, 3), mode='constant') for i in grayscaled_rgb]\n",
    "    grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "    grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "    embed = inception.predict(grayscaled_rgb_resized)\n",
    "    return embed\n",
    "\n",
    "# Image transformer\n",
    "datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, rotation_range=20, horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 10\n",
    "\n",
    "def image_a_b_gen(batch_size):\n",
    "    for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "        grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "        embed = create_inception_embedding(grayscaled_rgb)\n",
    "        lab_batch = rgb2lab(batch)\n",
    "        X_batch = lab_batch[:, :, :, 0]\n",
    "        X_batch = X_batch.reshape(X_batch.shape + (1,))\n",
    "        Y_batch = lab_batch[:, :, :, 1:] / 128\n",
    "        yield ([X_batch, embed], Y_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='mse')\n",
    "model.fit(image_a_b_gen(batch_size), epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_inception_embedding(grayscaled_rgb):\n",
    "#     grayscaled_rgb_resized = []\n",
    "#     for i in grayscaled_rgb:\n",
    "#         i = resize(i, (299, 299, 3), mode='constant')\n",
    "#         grayscaled_rgb_resized.append(i)\n",
    "#     grayscaled_rgb_resized = np.array(grayscaled_rgb_resized)\n",
    "#     grayscaled_rgb_resized = preprocess_input(grayscaled_rgb_resized)\n",
    "#     with inception.graph.as_default():\n",
    "#         embed = inception.predict(grayscaled_rgb_resized)\n",
    "#     return embed\n",
    "\n",
    "# # Image transformer\n",
    "# datagen = ImageDataGenerator(\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         rotation_range=20,\n",
    "#         horizontal_flip=True)\n",
    "\n",
    "# #Generate training data\n",
    "# batch_size = 10\n",
    "\n",
    "# def image_a_b_gen(batch_size):\n",
    "#     for batch in datagen.flow(Xtrain, batch_size=batch_size):\n",
    "#         grayscaled_rgb = gray2rgb(rgb2gray(batch))\n",
    "#         embed = create_inception_embedding(grayscaled_rgb)\n",
    "#         lab_batch = rgb2lab(batch)\n",
    "#         X_batch = lab_batch[:,:,:,0]\n",
    "#         X_batch = X_batch.reshape(X_batch.shape+(1,))\n",
    "#         Y_batch = lab_batch[:,:,:,1:] / 128\n",
    "#         yield ([X_batch, create_inception_embedding(grayscaled_rgb)], Y_batch)\n",
    "\n",
    "\n",
    "# #Train model      \n",
    "# model.compile(optimizer='rmsprop', loss='mse')\n",
    "# model.fit(image_a_b_gen(batch_size), epochs=1, steps_per_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 434ms/step\n",
      "4/4 [==============================] - 1s 136ms/step\n"
     ]
    }
   ],
   "source": [
    "color_me = []\n",
    "for filename in os.listdir(test_dir):\n",
    "    if filename == '.DS_Store':\n",
    "        continue\n",
    "    color_me.append(img_to_array(load_img(test_dir+filename)))\n",
    "color_me = np.array(color_me, dtype=float)\n",
    "gray_me = gray2rgb(rgb2gray(1.0/255*color_me))\n",
    "color_me_embed = create_inception_embedding(gray_me)\n",
    "color_me = rgb2lab(1.0/255*color_me)[:,:,:,0]\n",
    "color_me = color_me.reshape(color_me.shape+(1,))\n",
    "\n",
    "\n",
    "# Test model\n",
    "output = model.predict([color_me, color_me_embed])\n",
    "output = output * 128\n",
    "\n",
    "# Output colorizations\n",
    "# for i in range(len(output)):\n",
    "#     print(i)\n",
    "#     print(color_me.shape)\n",
    "#     cur = np.zeros((256, 256, 3))\n",
    "#     cur[:,:,0] = color_me[i][:,:,0]\n",
    "#     cur[:,:,1:] = output[i]\n",
    "#     imsave(\"result/img_\"+str(i)+\".png\", lab2rgb(cur))\n",
    "\n",
    "from skimage import io, color\n",
    "\n",
    "# this code fixes the above block of code but produces a warning\n",
    "# for i in range(len(output)):\n",
    "#     cur = np.zeros((256, 256, 3))\n",
    "#     cur[:,:,0] = color_me[i][:,:,0]\n",
    "#     cur[:,:,1:] = output[i]\n",
    "\n",
    "#     # Ensure the values are in the range 0-255\n",
    "#     cur = np.clip(cur, 0, 255)\n",
    "\n",
    "#     # Convert from LAB to RGB\n",
    "#     cur = color.lab2rgb(cur)\n",
    "\n",
    "#     # Convert to uint8 format\n",
    "#     cur = (cur * 255).astype(np.uint8)\n",
    "\n",
    "#     # Save the image\n",
    "#     io.imsave(\"result/img_\" + str(i) + \".png\", cur)\n",
    "\n",
    "\n",
    "# this fixes the warning as well\n",
    "for i in range(len(output)):\n",
    "    cur = np.zeros((256, 256, 3))\n",
    "    cur[:,:,0] = color_me[i][:,:,0]  # L channel\n",
    "    cur[:,:,1:] = output[i]  # a and b channels\n",
    "\n",
    "    # Clip the a and b channels to a range (-128, 127) that's more likely to convert cleanly\n",
    "    cur[:,:,1:] = np.clip(cur[:,:,1:], -128, 127)\n",
    "\n",
    "    # Convert from LAB to RGB, handling clipping internally\n",
    "    cur = color.lab2rgb(cur)\n",
    "\n",
    "    # Convert to uint8 format\n",
    "    cur = (cur * 255).astype(np.uint8)\n",
    "\n",
    "    # Save the image\n",
    "    io.imsave(\"result/img_\" + str(i) + \".png\", cur)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
